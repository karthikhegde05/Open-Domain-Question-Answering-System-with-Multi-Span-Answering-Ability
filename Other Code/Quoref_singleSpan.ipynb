{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Single_span_BERTQA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgCufFkbVBL7",
        "outputId": "a5ee661e-7726-4cd8-d0f1-9e3ef43d8788"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import zipfile\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/NLP Project/quoref-train-dev-v0.1.zip\"\n",
        "zip_ref = zipfile.ZipFile(data_path, 'r')\n",
        "zip_ref.extractall(\"/content\")\n",
        "zip_ref.close()\n",
        "\n",
        "\n",
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.44)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ4Bo_NuVORp"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForQuestionAnswering\n",
        "\n",
        "max_len = 512\n",
        "configuration = BertConfig()  # default parameters and configuration for BERT\n",
        "\n",
        "\n",
        "# Save the slow pretrained tokenizer\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # using the pretrained tokenizer\n",
        "save_path = \"/content/bert_base_uncased/\"\n",
        "#if not os.path.exists(save_path):\n",
        "    #os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "# Load the fast tokenizer from saved file\n",
        "tokenizer = BertWordPieceTokenizer(\"/content/bert_base_uncased/vocab.txt\", lowercase=True)\n",
        "\n",
        "\n",
        "# loading the data\n",
        "train_path = '/content/quoref-train-dev-v0.1/quoref-train-v0.1.json'\n",
        "eval_path = '/content/quoref-train-dev-v0.1/quoref-dev-v0.1.json'\n",
        "\n",
        "with open(train_path) as f:\n",
        "    raw_train_data = json.load(f)\n",
        "\n",
        "with open(eval_path) as f:\n",
        "    raw_eval_data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta8hFfLdVXh9"
      },
      "source": [
        "class DataInstance:\n",
        "\n",
        "  def __init__(self, question, context, start_word_idx, answer_text, all_answers):\n",
        "    self.question = question\n",
        "    self.context = context\n",
        "    self.start_word_idx = start_word_idx\n",
        "    self.answer_text = answer_text\n",
        "    self.all_answers = all_answers\n",
        "    self.skip = False\n",
        "\n",
        "\n",
        "  def preprocess(self):\n",
        "    context = self.context\n",
        "    question = self.question\n",
        "    answer_text = self.answer_text\n",
        "    start_word_idx = self.start_word_idx\n",
        "\n",
        "    # clean context, answer and question\n",
        "    context = \" \".join(str(context).split())\n",
        "    question = \" \".join(str(question).split())\n",
        "    answer = \" \".join(str(answer_text).split())\n",
        "\n",
        "    # Find end character index of answer in context\n",
        "    end_word_idx = start_word_idx + len(answer)\n",
        "    if (end_word_idx >= len(context)):\n",
        "      self.skip = True\n",
        "      return\n",
        "\n",
        "    # Mark the word indices in context that are in answer\n",
        "    is_word_in_ans = [0 for _ in range(len(context))]\n",
        "    for idx in range(start_word_idx, end_word_idx):\n",
        "      is_word_in_ans[idx] = 1\n",
        "\n",
        "    # Tokenize context\n",
        "    tokenized_context = tokenizer.encode(context)\n",
        "\n",
        "\n",
        "    # Find the words that were created from answer characters\n",
        "    # The offsets associated to each token. \n",
        "    # These offsets letâ€™s you slice the input string, and thus \n",
        "    # retrieve the original part that led to producing the corresponding token.\n",
        "    ans_token_idx = []\n",
        "    for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "      if (sum(is_word_in_ans[start:end]) > 0):\n",
        "        ans_token_idx.append(idx)\n",
        "\n",
        "\n",
        "    if len(ans_token_idx)==0:\n",
        "      self.skip = True\n",
        "      return\n",
        "\n",
        "    # Find the start and end token index for tokens from answer\n",
        "    start_token_idx = ans_token_idx[0]\n",
        "    end_token_idx = ans_token_idx[-1]\n",
        "\n",
        "    # tokenize question \n",
        "    tokenized_question = tokenizer.encode(question)\n",
        "\n",
        "    # Create inputs\n",
        "    #  The generated ID \n",
        "    #The IDs are the main input to a Language Model. \n",
        "    #They are the token indices, the numerical representations that a LM understands.\n",
        "    input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "    token_type_ids = [0 for _ in  range(len(tokenized_context.ids))] + [1 for _ in range(len(tokenized_question.ids[1:]))]\n",
        "    attention_mask = [1 for _ in range(len(input_ids))]\n",
        "\n",
        "\n",
        "    # Pad and create attention masks.\n",
        "    # Skip if truncation is needed\n",
        "    padding_length = max_len - len(input_ids)\n",
        "    if padding_length > 0:  # pad\n",
        "        input_ids = input_ids + ([0 for _ in range(padding_length)])\n",
        "        attention_mask = attention_mask + ([0 for _ in range(padding_length)])\n",
        "        token_type_ids = token_type_ids + ([0 for _ in range(padding_length)])\n",
        "    elif padding_length < 0:  # skip\n",
        "        self.skip = True\n",
        "        return\n",
        "\n",
        "    self.input_ids = input_ids\n",
        "    self.token_type_ids = token_type_ids\n",
        "    self.attention_mask = attention_mask\n",
        "    self.start_token_idx = start_token_idx\n",
        "    self.end_token_idx = end_token_idx\n",
        "    self.context_token_to_word = tokenized_context.offsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXRB2TouVZoa"
      },
      "source": [
        "def create_instance(raw_data):\n",
        "  data_instances = []\n",
        "  for item in raw_data[\"data\"]:\n",
        "    for para in item[\"paragraphs\"]:\n",
        "      context = para[\"context\"]\n",
        "      for qa in para[\"qas\"]:\n",
        "        question = qa[\"question\"]\n",
        "        answer_text = qa[\"answers\"][0][\"text\"] # considering only first answer\n",
        "        all_answers = [ans[\"text\"] for ans in qa[\"answers\"]]\n",
        "        start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "        quoref_instance = DataInstance(question, context, start_char_idx, answer_text, all_answers)\n",
        "        quoref_instance.preprocess()\n",
        "        data_instances.append(quoref_instance)\n",
        "  \n",
        "  return data_instances\n",
        "\n",
        "\n",
        "def create_input_targets(data_instances):\n",
        "  dataset_dict = {\n",
        "      \"input_ids\" : [],\n",
        "      \"token_type_ids\" : [],\n",
        "      \"attention_mask\" : [],\n",
        "      \"start_token_idx\": [],\n",
        "      \"end_token_idx\": [],\n",
        "  }\n",
        "\n",
        "  for item in data_instances:\n",
        "    if item.skip == False:\n",
        "      for key in dataset_dict:\n",
        "        dataset_dict[key].append(getattr(item, key))\n",
        "  for key in dataset_dict:\n",
        "    dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "  x = [\n",
        "       dataset_dict[\"input_ids\"],\n",
        "       dataset_dict[\"token_type_ids\"],\n",
        "       dataset_dict[\"attention_mask\"],\n",
        "  ]\n",
        "  y = [\n",
        "       dataset_dict[\"start_token_idx\"],\n",
        "       dataset_dict[\"end_token_idx\"],\n",
        "  ]\n",
        "\n",
        "  return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAydAg-wVb8b",
        "outputId": "8ad40aa6-7752-46f2-9dc6-a00b06a6cff2"
      },
      "source": [
        "train_instances = create_instance(raw_train_data)\n",
        "x_train, y_train = create_input_targets(train_instances)\n",
        "print(\"trainset size: {}\".format(len(train_instances)))\n",
        "\n",
        "eval_instances = create_instance(raw_eval_data)\n",
        "x_eval, y_eval = create_input_targets(eval_instances)\n",
        "print(\"evalset size: {}\".format(len(eval_instances)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainset size: 19399\n",
            "evalset size: 2418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B0BXgCyt_Tp",
        "outputId": "91ee073a-6abe-4c3e-a4da-a2071359aea4"
      },
      "source": [
        "print(x_eval[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1662, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw3elRNeVfPk"
      },
      "source": [
        "def create_model():\n",
        "  \n",
        "  #encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "  encoder = TFBertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "  # single span QA model\n",
        "  input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "  token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "  attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "  bert_output = encoder(\n",
        "                input_ids = input_ids,\n",
        "                token_type_ids = token_type_ids,\n",
        "                attention_mask = attention_mask,\n",
        "                return_dict= True\n",
        "              ) # outputs (start_logits, end_logits)\n",
        "\n",
        "  #start_logits = layers.Dense(1, name=\"start_logit\")(embedding)\n",
        "  #start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "  #end_logits = layers.Dense(1, name=\"end_logits\")(embedding)\n",
        "  #end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "  start_logits = bert_output[\"start_logits\"]\n",
        "  end_logits =  bert_output[\"end_logits\"]\n",
        "\n",
        "  start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "  end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "  model = keras.Model(\n",
        "      inputs = [input_ids, token_type_ids, attention_mask],\n",
        "      outputs = [start_probs, end_probs],\n",
        "  )\n",
        "\n",
        "  loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False) # cross entropy on probs of last layer\n",
        "  optimizer = keras.optimizers.Adam(lr=5e-5)\n",
        "  model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-wfG7J5Vhb3",
        "outputId": "862531f5-b66a-4a92-93d7-9c67f739dbc6"
      },
      "source": [
        "use_tpu = True\n",
        "if use_tpu:\n",
        "    # Create distribution strategy\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "    # Create model\n",
        "    with strategy.scope():\n",
        "        model = create_model()\n",
        "else:\n",
        "    model = create_model()\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.17.87.226:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.17.87.226:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.17.87.226:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.17.87.226:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
            "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
            "\n",
            "Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_16 (InputLayer)           [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_18 (InputLayer)           [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_for_question_answering_ TFQuestionAnsweringM 108893186   input_16[0][0]                   \n",
            "                                                                 input_18[0][0]                   \n",
            "                                                                 input_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 512)          0           tf_bert_for_question_answering_5[\n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 512)          0           tf_bert_for_question_answering_5[\n",
            "==================================================================================================\n",
            "Total params: 108,893,186\n",
            "Trainable params: 108,893,186\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmSNXeyfVjlE"
      },
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations\n",
        "    exclude = set(string.punctuation)\n",
        "    text = \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    # Remove articles\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    text = re.sub(regex, \" \", text)\n",
        "\n",
        "    # Remove extra white space\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "class ExactMatch(keras.callbacks.Callback):\n",
        "  def __init__(self, x_eval, y_eval):\n",
        "    self.x_eval = x_eval\n",
        "    self.y_eval = y_eval\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "    count = 0\n",
        "    eval_instances_no_skip = [ele for ele in eval_instances if ele.skip==False]\n",
        "    \n",
        "    for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "      quoref_eg = eval_instances_no_skip[idx]\n",
        "      offsets = quoref_eg.context_token_to_word\n",
        "      mat = np.outer(start, end)\n",
        "      pos = np.argmax(mat)\n",
        "      start = pos//mat.shape[1]\n",
        "      end = pos%mat.shape[1]\n",
        "      if (start >= len(offsets)):\n",
        "        continue\n",
        "      pred_word_start = offsets[start][0]\n",
        "      if (end < len(offsets)):\n",
        "        pred_word_end = offsets[end][1]\n",
        "        pred_ans = quoref_eg.context[pred_word_start:pred_word_end] # span of predicted ans with their indices # pred_word_start is the starting index, pred_word_end-1 is the ending index in context\n",
        "      else:\n",
        "        pred_ans = quoref_eg.context[pred_word_start:]\n",
        "\n",
        "      normalized_pred_ans = normalize_text(pred_ans)\n",
        "      normalized_true_ans = [normalize_text(_) for _ in quoref_eg.all_answers]\n",
        "      if (normalized_pred_ans in normalized_true_ans):\n",
        "        count += 1\n",
        "    acc_whole = count/len(self.y_eval[0])\n",
        "    acc  = count/len(eval_instances_no_skip)\n",
        "    print(f\"epoch={epoch+1}, EM_on_whole = {acc_whole:.2f}, exact match score={acc:.2f}\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL7Kv78vi4fV",
        "outputId": "740df39d-53a2-4960-ae43-6848e9f425d6"
      },
      "source": [
        "exact_match_callback = ExactMatch(x_eval, y_eval)\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    verbose=2,\n",
        "    batch_size=32,\n",
        "    callbacks=[exact_match_callback],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "410/410 - 138s - loss: 5.7800 - activation_6_loss: 2.7873 - activation_7_loss: 2.9927\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch=1, EM_on_whole = 0.37, exact match score=0.37\n",
            "\n",
            "Epoch 2/50\n",
            "410/410 - 55s - loss: 3.1741 - activation_6_loss: 1.4991 - activation_7_loss: 1.6750\n",
            "epoch=2, EM_on_whole = 0.43, exact match score=0.43\n",
            "\n",
            "Epoch 3/50\n",
            "410/410 - 55s - loss: 2.0283 - activation_6_loss: 0.9481 - activation_7_loss: 1.0803\n",
            "epoch=3, EM_on_whole = 0.43, exact match score=0.43\n",
            "\n",
            "Epoch 4/50\n",
            "410/410 - 55s - loss: 1.2891 - activation_6_loss: 0.5952 - activation_7_loss: 0.6940\n",
            "epoch=4, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 5/50\n",
            "410/410 - 55s - loss: 0.9207 - activation_6_loss: 0.4271 - activation_7_loss: 0.4936\n",
            "epoch=5, EM_on_whole = 0.44, exact match score=0.44\n",
            "\n",
            "Epoch 6/50\n",
            "410/410 - 55s - loss: 0.6703 - activation_6_loss: 0.3129 - activation_7_loss: 0.3574\n",
            "epoch=6, EM_on_whole = 0.44, exact match score=0.44\n",
            "\n",
            "Epoch 7/50\n",
            "410/410 - 55s - loss: 0.5489 - activation_6_loss: 0.2589 - activation_7_loss: 0.2900\n",
            "epoch=7, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 8/50\n",
            "410/410 - 55s - loss: 0.4220 - activation_6_loss: 0.1991 - activation_7_loss: 0.2230\n",
            "epoch=8, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 9/50\n",
            "410/410 - 55s - loss: 0.3880 - activation_6_loss: 0.1830 - activation_7_loss: 0.2050\n",
            "epoch=9, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 10/50\n",
            "410/410 - 55s - loss: 0.3148 - activation_6_loss: 0.1432 - activation_7_loss: 0.1715\n",
            "epoch=10, EM_on_whole = 0.44, exact match score=0.44\n",
            "\n",
            "Epoch 11/50\n",
            "410/410 - 55s - loss: 0.3167 - activation_6_loss: 0.1513 - activation_7_loss: 0.1654\n",
            "epoch=11, EM_on_whole = 0.42, exact match score=0.42\n",
            "\n",
            "Epoch 12/50\n",
            "410/410 - 55s - loss: 0.2817 - activation_6_loss: 0.1323 - activation_7_loss: 0.1494\n",
            "epoch=12, EM_on_whole = 0.47, exact match score=0.47\n",
            "\n",
            "Epoch 13/50\n",
            "410/410 - 55s - loss: 0.2469 - activation_6_loss: 0.1150 - activation_7_loss: 0.1319\n",
            "epoch=13, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 14/50\n",
            "410/410 - 55s - loss: 0.2276 - activation_6_loss: 0.1087 - activation_7_loss: 0.1189\n",
            "epoch=14, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 15/50\n",
            "410/410 - 55s - loss: 0.2084 - activation_6_loss: 0.1041 - activation_7_loss: 0.1043\n",
            "epoch=15, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 16/50\n",
            "410/410 - 55s - loss: 0.1977 - activation_6_loss: 0.0943 - activation_7_loss: 0.1034\n",
            "epoch=16, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 17/50\n",
            "410/410 - 55s - loss: 0.1905 - activation_6_loss: 0.0922 - activation_7_loss: 0.0983\n",
            "epoch=17, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 18/50\n",
            "410/410 - 55s - loss: 0.2093 - activation_6_loss: 0.1019 - activation_7_loss: 0.1075\n",
            "epoch=18, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 19/50\n",
            "410/410 - 55s - loss: 0.1871 - activation_6_loss: 0.0873 - activation_7_loss: 0.0998\n",
            "epoch=19, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 20/50\n",
            "410/410 - 55s - loss: 0.2109 - activation_6_loss: 0.1024 - activation_7_loss: 0.1085\n",
            "epoch=20, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 21/50\n",
            "410/410 - 55s - loss: 0.1589 - activation_6_loss: 0.0764 - activation_7_loss: 0.0826\n",
            "epoch=21, EM_on_whole = 0.44, exact match score=0.44\n",
            "\n",
            "Epoch 22/50\n",
            "410/410 - 55s - loss: 0.1469 - activation_6_loss: 0.0711 - activation_7_loss: 0.0758\n",
            "epoch=22, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 23/50\n",
            "410/410 - 55s - loss: 0.1523 - activation_6_loss: 0.0729 - activation_7_loss: 0.0793\n",
            "epoch=23, EM_on_whole = 0.44, exact match score=0.44\n",
            "\n",
            "Epoch 24/50\n",
            "410/410 - 55s - loss: 0.1445 - activation_6_loss: 0.0707 - activation_7_loss: 0.0738\n",
            "epoch=24, EM_on_whole = 0.47, exact match score=0.47\n",
            "\n",
            "Epoch 25/50\n",
            "410/410 - 55s - loss: 0.1348 - activation_6_loss: 0.0656 - activation_7_loss: 0.0693\n",
            "epoch=25, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 26/50\n",
            "410/410 - 55s - loss: 0.1322 - activation_6_loss: 0.0654 - activation_7_loss: 0.0668\n",
            "epoch=26, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 27/50\n",
            "410/410 - 55s - loss: 0.1391 - activation_6_loss: 0.0682 - activation_7_loss: 0.0709\n",
            "epoch=27, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 28/50\n",
            "410/410 - 55s - loss: 0.1589 - activation_6_loss: 0.0763 - activation_7_loss: 0.0826\n",
            "epoch=28, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 29/50\n",
            "410/410 - 55s - loss: 0.1276 - activation_6_loss: 0.0603 - activation_7_loss: 0.0674\n",
            "epoch=29, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 30/50\n",
            "410/410 - 55s - loss: 0.1254 - activation_6_loss: 0.0627 - activation_7_loss: 0.0628\n",
            "epoch=30, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 31/50\n",
            "410/410 - 56s - loss: 0.1258 - activation_6_loss: 0.0589 - activation_7_loss: 0.0669\n",
            "epoch=31, EM_on_whole = 0.47, exact match score=0.47\n",
            "\n",
            "Epoch 32/50\n",
            "410/410 - 55s - loss: 0.1052 - activation_6_loss: 0.0515 - activation_7_loss: 0.0536\n",
            "epoch=32, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 33/50\n",
            "410/410 - 55s - loss: 0.1431 - activation_6_loss: 0.0708 - activation_7_loss: 0.0723\n",
            "epoch=33, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 34/50\n",
            "410/410 - 55s - loss: 0.1387 - activation_6_loss: 0.0665 - activation_7_loss: 0.0723\n",
            "epoch=34, EM_on_whole = 0.47, exact match score=0.47\n",
            "\n",
            "Epoch 35/50\n",
            "410/410 - 55s - loss: 0.0956 - activation_6_loss: 0.0425 - activation_7_loss: 0.0531\n",
            "epoch=35, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 36/50\n",
            "410/410 - 55s - loss: 0.0982 - activation_6_loss: 0.0461 - activation_7_loss: 0.0521\n",
            "epoch=36, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 37/50\n",
            "410/410 - 55s - loss: 0.1072 - activation_6_loss: 0.0523 - activation_7_loss: 0.0549\n",
            "epoch=37, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 38/50\n",
            "410/410 - 55s - loss: 0.0968 - activation_6_loss: 0.0449 - activation_7_loss: 0.0519\n",
            "epoch=38, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 39/50\n",
            "410/410 - 55s - loss: 0.0902 - activation_6_loss: 0.0450 - activation_7_loss: 0.0452\n",
            "epoch=39, EM_on_whole = 0.44, exact match score=0.44\n",
            "\n",
            "Epoch 40/50\n",
            "410/410 - 55s - loss: 0.1116 - activation_6_loss: 0.0533 - activation_7_loss: 0.0583\n",
            "epoch=40, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 41/50\n",
            "410/410 - 55s - loss: 0.0991 - activation_6_loss: 0.0478 - activation_7_loss: 0.0513\n",
            "epoch=41, EM_on_whole = 0.47, exact match score=0.47\n",
            "\n",
            "Epoch 42/50\n",
            "410/410 - 55s - loss: 0.0971 - activation_6_loss: 0.0485 - activation_7_loss: 0.0486\n",
            "epoch=42, EM_on_whole = 0.47, exact match score=0.47\n",
            "\n",
            "Epoch 43/50\n",
            "410/410 - 55s - loss: 0.1051 - activation_6_loss: 0.0539 - activation_7_loss: 0.0511\n",
            "epoch=43, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 44/50\n",
            "410/410 - 55s - loss: 0.0930 - activation_6_loss: 0.0457 - activation_7_loss: 0.0474\n",
            "epoch=44, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 45/50\n",
            "410/410 - 55s - loss: 0.0851 - activation_6_loss: 0.0381 - activation_7_loss: 0.0470\n",
            "epoch=45, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 46/50\n",
            "410/410 - 55s - loss: 0.0918 - activation_6_loss: 0.0450 - activation_7_loss: 0.0468\n",
            "epoch=46, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 47/50\n",
            "410/410 - 55s - loss: 0.0805 - activation_6_loss: 0.0392 - activation_7_loss: 0.0413\n",
            "epoch=47, EM_on_whole = 0.45, exact match score=0.45\n",
            "\n",
            "Epoch 48/50\n",
            "410/410 - 55s - loss: 0.1128 - activation_6_loss: 0.0552 - activation_7_loss: 0.0576\n",
            "epoch=48, EM_on_whole = 0.42, exact match score=0.42\n",
            "\n",
            "Epoch 49/50\n",
            "410/410 - 55s - loss: 0.1051 - activation_6_loss: 0.0521 - activation_7_loss: 0.0531\n",
            "epoch=49, EM_on_whole = 0.46, exact match score=0.46\n",
            "\n",
            "Epoch 50/50\n",
            "410/410 - 55s - loss: 0.0779 - activation_6_loss: 0.0362 - activation_7_loss: 0.0417\n",
            "epoch=50, EM_on_whole = 0.47, exact match score=0.47\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f31b8af1f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7uGPqPm2mTt",
        "outputId": "59f228c6-a4de-48e8-add1-ad251d07be52"
      },
      "source": [
        "pred_start, pred_end = model.predict(x_eval)\n",
        "count = 0\n",
        "eval_instances_no_skip = [ele for ele in eval_instances if ele.skip==False]\n",
        "    \n",
        "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "  quoref_eg = eval_instances_no_skip[idx]\n",
        "  offsets = quoref_eg.context_token_to_word\n",
        "  mat = np.outer(start, end)\n",
        "  pos = np.argmax(mat)\n",
        "  start = pos//mat.shape[1]\n",
        "  end = pos%mat.shape[1]\n",
        "  if (start >= len(offsets)):\n",
        "    continue\n",
        "  pred_word_start = offsets[start][0]\n",
        "  if (end < len(offsets)):\n",
        "    pred_word_end = offsets[end][1]\n",
        "    pred_ans = quoref_eg.context[pred_word_start:pred_word_end] # span of predicted ans with their indices # pred_word_start is the starting index, pred_word_end-1 is the ending index in context\n",
        "  else:\n",
        "    pred_ans = quoref_eg.context[pred_word_start:]\n",
        "\n",
        "  normalized_pred_ans = normalize_text(pred_ans)\n",
        "  normalized_true_ans = [normalize_text(_) for _ in quoref_eg.all_answers]\n",
        "  if (normalized_pred_ans in normalized_true_ans):\n",
        "    count += 1\n",
        "\n",
        "\n",
        "acc_whole = count/len(y_eval[0])\n",
        "acc  = count/len(eval_instances_no_skip)\n",
        "print(f\"EM count = {count}, EM_on_whole = {acc_whole:.2f}, exact match score={acc:.2f}\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EM count = 776, EM_on_whole = 0.47, exact match score=0.47\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}